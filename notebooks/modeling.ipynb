{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# pvlib imports\n",
    "import pvlib\n",
    "from pvlib.location import Location\n",
    "from pvlib.pvsystem import PVSystem\n",
    "from pvlib.modelchain import ModelChain\n",
    "from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\n",
    "from pvlib import irradiance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\nik\\Desktop\\Berkeley_Projects\\net-load-forecasting\n"
     ]
    }
   ],
   "source": [
    "# Set working directory\n",
    "os.chdir(r\"..\") # should be the git repo root directory\n",
    "print(\"Current working directory: \" + os.getcwd())\n",
    "repo_name = 'net-load-forecasting'\n",
    "assert os.getcwd()[-len(repo_name):] == \"net-load-forecasting\", \"Working directory is not the git repo root directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_path = os.path.join(os.getcwd(),'data','clean_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>component</th>\n",
       "      <th>net_community_load</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-08-27 00:00:00</th>\n",
       "      <td>2973.965877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-27 00:15:00</th>\n",
       "      <td>2678.502833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-27 00:30:00</th>\n",
       "      <td>2776.943558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-27 00:45:00</th>\n",
       "      <td>2975.639441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-27 01:00:00</th>\n",
       "      <td>3332.794556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29 22:45:00</th>\n",
       "      <td>4587.582423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29 23:00:00</th>\n",
       "      <td>4447.781363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29 23:15:00</th>\n",
       "      <td>3685.346414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29 23:30:00</th>\n",
       "      <td>3666.789860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29 23:45:00</th>\n",
       "      <td>3456.915614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82176 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "component            net_community_load\n",
       "time                                   \n",
       "2014-08-27 00:00:00         2973.965877\n",
       "2014-08-27 00:15:00         2678.502833\n",
       "2014-08-27 00:30:00         2776.943558\n",
       "2014-08-27 00:45:00         2975.639441\n",
       "2014-08-27 01:00:00         3332.794556\n",
       "...                                 ...\n",
       "2016-12-29 22:45:00         4587.582423\n",
       "2016-12-29 23:00:00         4447.781363\n",
       "2016-12-29 23:15:00         3685.346414\n",
       "2016-12-29 23:30:00         3666.789860\n",
       "2016-12-29 23:45:00         3456.915614\n",
       "\n",
       "[82176 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_hdf(os.path.join(clean_data_path, \"data_net_load_forecasting.h5\"), key='15min/netload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracies_per_month(df, model_type, metrics:list = None):\n",
    "\n",
    "    accuracy_dictionary = {}\n",
    "\n",
    "    start_date = df.index[0]\n",
    "    test_begin = df.index[-1] - pd.Timedelta(days = 30)\n",
    "    for month in range(1,12):\n",
    "        train_end = start_date + pd.Timedelta(weeks = 4*month)\n",
    "\n",
    "        assert train_end < test_begin, \"Reduce the number of months, so a test period remains.\"\n",
    "        train = df[start_date:train_end]\n",
    "\n",
    "        test = df[test_begin:]\n",
    "\n",
    "\n",
    "        scaler_ml_features = MinMaxScaler()\n",
    "        scaler_ml_target = MinMaxScaler()\n",
    "\n",
    "        X_train, y_train = scaler_ml_features.fit_transform(train.iloc[:,1:]), scaler_ml_target.fit_transform(train.iloc[:,:1])\n",
    "        X_test, y_test = scaler_ml_features.transform(test.iloc[:,1:]), scaler_ml_target.transform(test.iloc[:,:1])\n",
    "\n",
    "\n",
    "        model = model_type()\n",
    "        model.fit(X = X_train, y = y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        gt_unscaled = scaler_ml_target.inverse_transform(y_test.reshape(-1,1))\n",
    "        predictions_unscaled = scaler_ml_target.inverse_transform(predictions.reshape(-1,1))\n",
    "\n",
    "        df_y_test = pd.DataFrame({\"ground_truth\":gt_unscaled.squeeze(), \"predictions\": predictions_unscaled.squeeze()}, index = test.index)\n",
    "\n",
    "        rmse = mean_squared_error(y_true=df_y_test[\"ground_truth\"], y_pred= df_y_test[\"predictions\"], squared=False) / test.max()[0]\n",
    "        r2 = r2_score(y_true=df_y_test[\"ground_truth\"], y_pred= df_y_test[\"predictions\"])\n",
    "\n",
    "        accuracy_dictionary[month] = rmse, r2\n",
    "\n",
    "    df_accuracy = pd.DataFrame(accuracy_dictionary, index = [\"rmse\", \"r2\"]).T\n",
    "\n",
    "    return df_accuracy, df_y_test\n",
    "\n",
    "\n",
    "\n",
    "def physical_profile(row, df_irr):\n",
    "    idx, latitude, longitude, tilt, azimuth, capacity = row\n",
    "\n",
    "    temperature_model_parameters = TEMPERATURE_MODEL_PARAMETERS[\"sapm\"][\n",
    "        \"open_rack_glass_glass\"\n",
    "    ]\n",
    "\n",
    "    location = Location(latitude=latitude, longitude=longitude)\n",
    "\n",
    "    pvwatts_system = PVSystem(\n",
    "        surface_tilt=tilt,\n",
    "        surface_azimuth=azimuth,\n",
    "        module_parameters={\"pdc0\": capacity, \"gamma_pdc\": -0.004},\n",
    "        inverter_parameters={\"pdc0\": capacity},\n",
    "        temperature_model_parameters=temperature_model_parameters,\n",
    "    )\n",
    "\n",
    "    mc = ModelChain(\n",
    "        pvwatts_system, location, aoi_model=\"physical\", spectral_model=\"no_loss\" #these are my model chain assumptions\n",
    "    )\n",
    "    mc.run_model(df_irr)\n",
    "    results = mc.results.ac\n",
    "\n",
    "    df_results = pd.Series(results)\n",
    "    df_results.index = df_results.index.tz_localize(None)\n",
    "    df_results.index.name = \"timestamp\"\n",
    "    df_results.name = str(tilt) + \";\" + str(azimuth)\n",
    "    df_results.columns = [str(idx) + \"_physical_profile\"]\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "def pv_day_filter(data, lat, lon, tilt, azimuth, timesteplen):\n",
    "\n",
    "    site = Location(lat, lon)\n",
    "    index = data.index\n",
    "    times = pd.date_range(index[0], index[-1], freq=str(timesteplen) + \"T\")\n",
    "    clearsky = site.get_clearsky(times)\n",
    "    solar_position = site.get_solarposition(times=times)\n",
    "    # Use the get_total_irradiance function to transpose the GHI to POA\n",
    "    POA_irradiance = irradiance.get_total_irradiance(\n",
    "        surface_tilt=tilt,\n",
    "        surface_azimuth=azimuth,\n",
    "        dni=clearsky[\"dni\"],\n",
    "        ghi=clearsky[\"ghi\"],\n",
    "        dhi=clearsky[\"dhi\"],\n",
    "        solar_zenith=solar_position[\"apparent_zenith\"],\n",
    "        solar_azimuth=solar_position[\"azimuth\"],\n",
    "    )\n",
    "\n",
    "    day_index = POA_irradiance[POA_irradiance[\"poa_global\"] > 0].index\n",
    "\n",
    "    data_day_values = data.reindex(day_index).dropna()\n",
    "\n",
    "    return data_day_values\n",
    "\n",
    "\n",
    "\n",
    "def get_df_compare_physical(df_meta, df_power, df_irr, test_period_index):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df_irr must have columns ['ghi', 'dni', 'dhi']\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df_compares = []\n",
    "    timesteplen = int(test_period_index.freq.delta.seconds/60) # in minutes\n",
    "    \n",
    "    for system_id in df_meta.index:\n",
    "        df_meta_data_id = df_meta.loc[system_id]\n",
    "        idx, lat, lon, tilt, azimuth, _ = df_meta_data_id.values\n",
    "        \n",
    "        df_irr = df_irr.reindex(test_period_index).dropna()\n",
    "        df_physical_profile = physical_profile(df_meta_data_id, df_irr).to_frame(idx).resample(\"15T\").mean().fillna(method = \"bfill\", limit = 4) \n",
    "        df_physical_profile = pv_day_filter(df_physical_profile, lat, lon, tilt, azimuth, timesteplen)\n",
    "\n",
    "    \n",
    "        df_power_id = df_power[[str(system_id)]].resample(str(timesteplen) + \"T\").mean()\n",
    "\n",
    "        df_compare = pd.merge(df_power_id, df_physical_profile, left_index=True, right_index=True, how = \"right\").fillna(method = \"bfill\", limit= 4).dropna()\n",
    "        df_physical_profile = pv_day_filter(df_compare, lat, lon, tilt, azimuth, timesteplen)\n",
    "        df_compares.append(df_compare.reindex(test_period_index).dropna())\n",
    "\n",
    "    return df_compares\n",
    "\n",
    "\n",
    "def calc_accuracies_from_df_compares(df_compares:list, metrics:list, lat, lon, tilt, azimuth, timesteplen):\n",
    "\n",
    "    \"\"\"\n",
    "    Inputs are a list of pd.DataFrames = [[df_predictions, df_ground_truth],[df_predictions, df_ground_truth], .... ] and a list of error metrics from sklearn.metrics\n",
    "\n",
    "    Note that the mse will be reinterpreted as the nRMSE. This is because of the specific problem that has been analysis, i.e., the different capacities of pv systems. \n",
    "\n",
    "    Outputs are the error scores for each df in the dfs list for each error metric in a table that looks like:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    metrics_names = [metric.__name__ for metric in metrics]\n",
    "    df_acc = pd.DataFrame(columns = metrics_names, index = range(len(df_compares)))\n",
    "\n",
    "\n",
    "    system_ids = []\n",
    "    for i, df_compare in enumerate(df_compares):\n",
    "\n",
    "\n",
    "        df_compare = pv_day_filter(df_compare, lat, lon, tilt, azimuth, timesteplen)\n",
    "        system_ids.append(int(df_compare.columns[0]))\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric_name = metric.__name__\n",
    "            score = metric(df_compare.iloc[:,:1],df_compare.iloc[:,1:])\n",
    "            if metric_name == \"mean_squared_error\":\n",
    "                score = np.sqrt(score)\n",
    "        \n",
    "            df_acc.loc[i, metric_name] = score\n",
    "\n",
    "\n",
    "    #transforming the mse into the rmse\n",
    "    if \"mean_squared_error\" in metrics_names:\n",
    "\n",
    "        df_acc[\"root_mean_squared_error\"] = np.sqrt(df_acc[\"mean_squared_error\"].astype(\"float32\"))\n",
    "\n",
    "        df_acc.drop([\"mean_squared_error\"], axis = 1, inplace=True)\n",
    "\n",
    "    df_acc.index = system_ids\n",
    "\n",
    "    return df_acc\n",
    "\n",
    "\n",
    "\n",
    "def realistic_case_round(value:int, interval:int):\n",
    "\n",
    "    \"\"\"\n",
    "    Rounding the tilt to interval'th degrees.\n",
    "    \"\"\"\n",
    "    rounded_value = round(value / interval) * interval\n",
    "    \n",
    "    return rounded_value\n",
    "\n",
    "\n",
    "\n",
    "def make_df_plot_from_stats(stats_d):\n",
    "    df_plot = pd.DataFrame(stats_d).T.round(4)\n",
    "    df_plot.columns = [\"mean\", \"std\"]\n",
    "    return df_plot\n",
    "\n",
    "\n",
    "def metric_skill_score(df_accs_baseline:pd.DataFrame, df_accs_model:pd.DataFrame, scenario_name:str):\n",
    "    \"\"\"\n",
    "    Calculated as follows\n",
    "    1 - (score_model / score_reference) and then we average over all systems\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    df_skill_scores = (1 - df_accs_model.div(df_accs_baseline))\n",
    "    mean = df_skill_scores.mean().to_frame(\"expected value_\" + scenario_name)\n",
    "\n",
    "    std = df_skill_scores.std().to_frame(\"standard deviation_\" + scenario_name)\n",
    "\n",
    "    out = pd.concat([mean, std], axis = 1)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def random_angle_init(\n",
    "                        df_meta:pd.DataFrame, \n",
    "                        df_power:pd.DataFrame,\n",
    "                        df_irr:pd.DataFrame,\n",
    "                        df_acc_baseline:pd.DataFrame,\n",
    "                        test_period_index: pd.date_range, \n",
    "                        lat, lon, tilt, azimuth, timesteplen,\n",
    "                        error_metric_list:list,\n",
    "                        tilt_bounds:tuple = (10,60), \n",
    "                        azimuth_bounds:tuple = (0,360),\n",
    "                        scenario_name = \"P3\"\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Use for scenarios in which angles are guessed by the aggregator. They are drawn *iters*-times from a uniform distribution for all systems within the optionally specified bounds.\n",
    "    Note: df_power and df_irr need to be in the global namespace of the file. \n",
    "    \"\"\"\n",
    "\n",
    "    df_meta_random = copy.deepcopy(df_meta)\n",
    "    random_tilts = np.random.randint(tilt_bounds[0], tilt_bounds[1], (df_meta_random.shape[0]))\n",
    "    random_azimuths = np.random.randint(azimuth_bounds[0], azimuth_bounds[1], (df_meta_random.shape[0]))\n",
    "    df_meta_random[\"tilt\"] = random_tilts\n",
    "    df_meta_random[\"azimuth\"] = random_azimuths\n",
    "    df_meta_compare = get_df_compare_physical(df_meta_random, df_power, df_irr, test_period_index= test_period_index)\n",
    "    df_meta_accs = calc_accuracies_from_df_compares(df_meta_compare, error_metric_list,lat, lon, tilt, azimuth, timesteplen)\n",
    "    df_skill_score = metric_skill_score(df_acc_baseline, df_meta_accs, scenario_name)\n",
    "    \n",
    "    return df_skill_score\n",
    "\n",
    "\n",
    "def drop_duplicate_index(df, axis=0):\n",
    "    \"\"\"\n",
    "    Removes all duplicate columns or index items of a pd.DataFrame. (Keeps first)\n",
    "    \"\"\"\n",
    "    if axis == 0:\n",
    "        df = df.loc[~df.columns.duplicated(keep=\"last\"),:]\n",
    "    elif axis == 1:\n",
    "        df = df.loc[:,~df.columns.duplicated(keep=\"last\")]\n",
    "    else:\n",
    "        raise ValueError(\"Make sure axis is either 0 (index) or 1 (column)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def acc_bar_plot(df_plot: pd.DataFrame):\n",
    "    today = datetime.datetime.today()\n",
    "    for i in range(df_plot.shape[0]):\n",
    "        means = df_plot.iloc[i].filter(like = \"exp\").sort_index()\n",
    "        stds = df_plot.iloc[i].filter(like = \"dev\").sort_index()\n",
    "        \n",
    "        # finding the scenario names for the x axis labels\n",
    "        idx = means.index\n",
    "        result = [re.findall(r'_(.*)', s)[0] for s in idx]\n",
    "        \n",
    "        \n",
    "        fig, ax = plt.subplots(figsize = (12,8))\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.grid(axis = \"y\")\n",
    "        ax.bar(x = result, height = means, yerr =stds, capsize = 10)\n",
    "        ax.set_ylim(0,1.5*means.max())\n",
    "        acc_score = means.name.replace(\"_\", \" \").title() + \" Skill-Score\"\n",
    "        ax.set_ylabel(acc_score)\n",
    "        fig.savefig(f\"../../../Results/Plots/{acc_score}_{today}_persistence_ref.pdf\")\n",
    "\n",
    "\n",
    "def euro_cost_metric(gt, prediction, prices):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def get_df_compare_physical_new(df_meta, df_power, df_irr, test_period_index):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df_irr must have columns ['ghi', 'dni', 'dhi']\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    list_df_physical = []\n",
    "    timesteplen = int(test_period_index.freq.delta.seconds/60) # in minutes\n",
    "    \n",
    "    for system_id in df_meta.index:\n",
    "        df_meta_data_id = df_meta.loc[system_id]\n",
    "        idx, lat, lon, tilt, azimuth, _ = df_meta_data_id.values\n",
    "        \n",
    "        df_irr = df_irr.reindex(test_period_index).dropna()\n",
    "        df_physical_profile = physical_profile(df_meta_data_id, df_irr).to_frame(idx).resample(\"15T\").mean().fillna(method = \"bfill\", limit = 4) \n",
    "        df_physical_profile = pv_day_filter(df_physical_profile, lat, lon, tilt, azimuth, timesteplen)\n",
    "\n",
    "        list_df_physical.append(df_physical_profile)\n",
    "\n",
    "    \n",
    "    df_power_ = df_power.resample(str(timesteplen) + \"T\").mean()\n",
    "    df_physical = pd.concat(list_df_physical, axis = 1).sum(axis = 1).to_frame(\"power_physical_model\")\n",
    "    df_compare = pd.merge(df_power_, df_physical, left_index=True, right_index=True, how = \"left\")\n",
    "    df_compare = df_compare.fillna(0)\n",
    "\n",
    "    return df_compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netload",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
