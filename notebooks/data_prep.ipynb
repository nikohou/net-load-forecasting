{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import plotly.express as px\n",
    "import h5py\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from scipy import stats\n",
    "# pvlib imports\n",
    "import pvlib\n",
    "from pvlib.pvsystem import PVSystem\n",
    "from pvlib.location import Location\n",
    "from pvlib.modelchain import ModelChain\n",
    "from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\n",
    "\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\nik\\Desktop\\Berkeley_Projects\\net-load-forecasting\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set working directory\n",
    "os.chdir(r\"..\") # should be the git repo root directory\n",
    "print(\"Current working directory: \" + os.getcwd())\n",
    "repo_name = 'net-load-forecasting'\n",
    "assert os.getcwd()[-len(repo_name):] == \"net-load-forecasting\", \"Working directory is not the git repo root directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(os.getcwd(),'data','clean_data')\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    print(\"Created directory: \" + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom functions\n",
    "from utils.utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imports & Cleaning for the Project "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x2460; Energy Community Load Data - Germany\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5642902.svg)](https://doi.org/10.5281/zenodo.5642902) \n",
    "\n",
    "Note: Due to their size we have not included the datasets in the repo, but the above link will allow you to download them, np.\n",
    "\n",
    "We have downloaded the load data for [2018,2019,2020] in [1 minute*, 15 minutes] resolution:\n",
    "\n",
    "* 2018_data_1min.zip, 2019_data_1min.zip, 2020_data_1min.zip\n",
    "* 2018_data_15min.zip, 2019_data_15min.zip, 2020_data_15min.zip,\n",
    "\n",
    "The goal here is to import them, select the useful data, impute missing data where plausible and aggregate to one community for both temporal resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h5py extraction\n",
    "resolutions = [\n",
    "                #'1min',\n",
    "                '15min']\n",
    "\n",
    "df_load_per_resolution = {}\n",
    "store = pd.HDFStore(os.path.join(save_path, \"df_load_per_resolution.h5\"))\n",
    "\n",
    "for resolution in resolutions:\n",
    "\n",
    "    dfs_load = []\n",
    "    for year in [2018]:\n",
    "\n",
    "        with zipfile.ZipFile(f\"data/raw_data/load/{year}_data_{resolution}.zip\") as zip_file:\n",
    "\n",
    "            hdf5_file = zip_file.open(f\"{year}_data_{resolution}.hdf5\")\n",
    "\n",
    "            f = h5py.File(hdf5_file)\n",
    "            group_no_pv = f[\"NO_PV\"] #Only regard those profiles that are not mixed with PV generation\n",
    "            dfs = {}\n",
    "            community_members = [ # these are the households with reliable data for the considered duration\n",
    "                        'SFH3', 'SFH4', 'SFH5', 'SFH9', 'SFH10',\n",
    "                        'SFH12', 'SFH16','SFH18','SFH19', 'SFH21',\n",
    "                        'SFH22', 'SFH23', 'SFH27', 'SFH28', 'SFH29',\n",
    "                        'SFH30', 'SFH31','SFH32', 'SFH36', 'SFH38'\n",
    "                        ]\n",
    "            \n",
    "            for member in community_members:\n",
    "                table = f[\"NO_PV\"][member][\"HOUSEHOLD\"][\"table\"][:]\n",
    "                df = pd.DataFrame(table).dropna().set_index(\"index\")[[\"P_TOT\"]]\n",
    "                df.index = pd.to_datetime(df.index, unit = \"s\")\n",
    "                dfs[member] = df\n",
    "                break\n",
    "\n",
    "            df_load = pd.concat(dfs, axis=1).sum(axis=1).to_frame('total_load')\n",
    "            dfs_load.append(df_load)\n",
    "\n",
    "    df_load_total = pd.concat(dfs_load, axis=0)\n",
    "    df_load_per_resolution[resolution] = df_load_total\n",
    "    store.put(f'{resolution}/data', df_load_total, format='table')\n",
    "store.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_load</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-02 14:30:00</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-02 14:45:00</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-02 15:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-02 15:15:00</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-02 15:30:00</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 22:45:00</th>\n",
       "      <td>237.117112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:00:00</th>\n",
       "      <td>231.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:15:00</th>\n",
       "      <td>231.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:30:00</th>\n",
       "      <td>231.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:45:00</th>\n",
       "      <td>231.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23307 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     total_load\n",
       "index                          \n",
       "2018-05-02 14:30:00    0.000000\n",
       "2018-05-02 14:45:00    0.000000\n",
       "2018-05-02 15:00:00    0.000000\n",
       "2018-05-02 15:15:00    0.000000\n",
       "2018-05-02 15:30:00    0.000000\n",
       "...                         ...\n",
       "2018-12-31 22:45:00  237.117112\n",
       "2018-12-31 23:00:00  231.250000\n",
       "2018-12-31 23:15:00  231.250000\n",
       "2018-12-31 23:30:00  231.250000\n",
       "2018-12-31 23:45:00  231.250000\n",
       "\n",
       "[23307 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_load_per_resolution['15min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The file 'c:\\Users\\nik\\Desktop\\Berkeley_Projects\\net-load-forecasting\\data\\clean_data\\df_load_per_resolution.h5' is already opened, but not in read-only mode (as requested).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pd\u001b[39m.\u001b[39;49mread_hdf(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(save_path, \u001b[39m\"\u001b[39;49m\u001b[39mdf_load_per_resolution.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m), key\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m15min\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\nik\\miniconda3\\envs\\netload\\Lib\\site-packages\\pandas\\io\\pytables.py:420\u001b[0m, in \u001b[0;36mread_hdf\u001b[1;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exists:\n\u001b[0;32m    418\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00mpath_or_buf\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 420\u001b[0m store \u001b[39m=\u001b[39m HDFStore(path_or_buf, mode\u001b[39m=\u001b[39;49mmode, errors\u001b[39m=\u001b[39;49merrors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    421\u001b[0m \u001b[39m# can't auto open/close if we are using an iterator\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[39m# so delegate to the iterator\u001b[39;00m\n\u001b[0;32m    423\u001b[0m auto_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nik\\miniconda3\\envs\\netload\\Lib\\site-packages\\pandas\\io\\pytables.py:579\u001b[0m, in \u001b[0;36mHDFStore.__init__\u001b[1;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fletcher32 \u001b[39m=\u001b[39m fletcher32\n\u001b[0;32m    578\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filters \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 579\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(mode\u001b[39m=\u001b[39;49mmode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nik\\miniconda3\\envs\\netload\\Lib\\site-packages\\pandas\\io\\pytables.py:731\u001b[0m, in \u001b[0;36mHDFStore.open\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    726\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot open HDF5 file, which is already opened, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    727\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39meven in read-only mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    728\u001b[0m     )\n\u001b[0;32m    729\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m--> 731\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m tables\u001b[39m.\u001b[39;49mopen_file(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nik\\miniconda3\\envs\\netload\\Lib\\site-packages\\tables\\file.py:284\u001b[0m, in \u001b[0;36mopen_file\u001b[1;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[39m# 'r' is incompatible with everything except 'r' itself\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m omode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is already opened, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot in read-only mode (as requested).\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m filename)\n\u001b[0;32m    287\u001b[0m \u001b[39m# 'a' and 'r+' are compatible with everything except 'r'\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m omode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: The file 'c:\\Users\\nik\\Desktop\\Berkeley_Projects\\net-load-forecasting\\data\\clean_data\\df_load_per_resolution.h5' is already opened, but not in read-only mode (as requested)."
     ]
    }
   ],
   "source": [
    "pd.read_hdf(os.path.join(save_path, \"df_load_per_resolution.h5\"), key='15min')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x2461; PV Power Data - Netherlands\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6906504.svg)](https://doi.org/10.5281/zenodo.6906504)\n",
    "\n",
    "We have downloaded the 'filtered_pv_power_measurements_ac.csv' file\n",
    "\n",
    "Note: While meta data is available for download, exact locations of individual PV systems are not included. Since one of the key research questions of this project is to investigate the impact of using exact locations in modeling, we recieved special permission to use the longitude and latitude of PV systems. Unfortunately we cannot share these here. The rest of the code is executable with the boundary box locations provided in the 'metadata.csv', however. \n",
    "\n",
    "Note: Due to their size we have not included the datasets in the repo, but the above link will allow you to download them, np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv = pd.read_csv(os.path.join(os.getcwd(), 'data', 'raw_data', 'pv', 'filtered_pv_power_measurements_ac.csv'), index_col=0, parse_dates=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x2462; Irradiance Data - Netherlands\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Merging & Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netload",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
